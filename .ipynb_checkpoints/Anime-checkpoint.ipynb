{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"position: relative; padding: 20px;\">\n",
    "\n",
    "  <h1 style=\"font-family: 'Comic Sans MS', cursive; font-size: 36px; color: #ff4d6d; \n",
    "             text-shadow: 2px 2px 5px black;\">\n",
    "    ‚ú® Anime Recommender System using Unsupervised Learning ‚ú®\n",
    "  </h1>\n",
    "\n",
    "  <img src=\"https://th.bing.com/th/id/OIP.rcZi51S5pAjcUw2X_B9sxAHaHa?w=174&h=180&c=7&r=0&o=5&pid=1.7\" alt=\"Anime Recommender\" width=\"400\" \n",
    "       style=\"border-radius: 15px; box-shadow: 0 0 15px rgba(255, 77, 109, 0.7); margin-top: 10px;\">\n",
    "  \n",
    "  <p style=\"font-size: 18px; font-style: italic; color: #333;\">\n",
    "    Discover your next favorite anime with machine learning magic! üéå‚ú®\n",
    "  </p>\n",
    "\n",
    "  <div id=\"popup\" style=\"position: absolute; top: -40px; right: 10px; \n",
    "                         background: #ff4d6d; color: white; padding: 10px 20px; \n",
    "                         border-radius: 5px; font-weight: bold; display: none;\">\n",
    "    New Recommendation! üéâ\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "  setTimeout(() => {\n",
    "    document.getElementById('popup').style.display = 'block';\n",
    "  }, 2000);\n",
    "</script>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üìú Table of Contents üéå\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "- üìå [**Introduction**](#1-introduction)  \n",
    "  - üîπ [Problem Statement](#11-problem-statement)  \n",
    "  - üîπ [Objectives](#12-objectives)  \n",
    "- üìå [**Importing Packages**](#2-importing-packages)  \n",
    "- üìå [**Data Loading and Inspection**](#3-data-loading-and-inspection)  \n",
    "- üìå [**Data Cleaning**](#4-data-cleaning)  \n",
    "- üìå [**Exploratory Data Analysis (EDA)**](#5-exploratory-data-analysis-eda)  \n",
    "- üìå [**Data Preprocessing**](#6-data-preprocessing)  \n",
    "- üìå [**Model Development**](#7-model-development)  \n",
    "- üìå [**Model Evaluation**](#8-model-evaluation)  \n",
    "- üìå [**Model Deployment**](#9-model-deployment)  \n",
    "- üìå [**Conclusion and Recommendations**](#10-conclusion-and-recommendations)  \n",
    "- üìå [**References**](#11-references)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üåü 1. Introduction üåü\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "<a id=\"1-introduction\"></a>  \n",
    "\n",
    "### **1.1 Problem Statement**  \n",
    "<a id=\"11-problem-statement\"></a>  \n",
    "\n",
    "With the rapid growth of anime streaming platforms, users often struggle to find new anime that align with their preferences. A **recommender system** can help solve this problem by analyzing user-anime interactions and suggesting relevant content.  \n",
    "\n",
    "In this project, we will develop an **unsupervised learning-based anime recommender system** using clustering techniques. The goal is to provide **personalized recommendations** based on user behavior and anime attributes.  \n",
    "\n",
    "### **1.2 Objectives**  \n",
    "<a id=\"12-objectives\"></a>  \n",
    "\n",
    "‚úÖ Develop an **unsupervised learning** model to recommend anime based on user preferences.  \n",
    "‚úÖ Perform **data cleaning, preprocessing, and exploratory data analysis (EDA)** to understand patterns in the dataset.  \n",
    "‚úÖ Implement **clustering algorithms** (e.g., K-Means, DBSCAN) for grouping similar anime.  \n",
    "‚úÖ Evaluate the performance of the recommendation system.  \n",
    "‚úÖ Submit predictions to the **Kaggle Challenge Leaderboard** for evaluation.  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üì¶ 2. Importing Packages üöÄ\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "<a id=\"2-importing-packages\"></a>  \n",
    "\n",
    "In this section, we import the necessary packages to execute our data science workflow. These packages include essential tools for **data manipulation, visualization, preprocessing, and machine learning**.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from datetime import datetime  \n",
    "\n",
    "# Visualization Libraries  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "import plotly.express as px  \n",
    "import plotly.graph_objects as go  \n",
    "import plotly.figure_factory as ff  \n",
    "from plotly.offline import init_notebook_mode, iplot  \n",
    "\n",
    "# Text Processing & Feature Extraction  \n",
    "# Display a decorative section header\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from scipy import stats  \n",
    "\n",
    "# Machine Learning & Dimensionality Reduction  \n",
    "from sklearn import (manifold, decomposition, ensemble, discriminant_analysis,  \n",
    "                     random_projection, preprocessing, datasets)  \n",
    "\n",
    "# Similarity & Error Metrics  \n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import mean_squared_error  \n",
    "\n",
    "# Timing Execution  \n",
    "from time import time  \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# from scipy.sparse import csr_matrix\n",
    "from surprise import Dataset, Reader, SVD, BaselineOnly, CoClustering\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "from math import sqrt\n",
    "# import implicit\n",
    "\n",
    "# Ignore Warnings for Clean Output  \n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üìÇ 3. Data Loading and Inspection üßê\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "<a id=\"3-data-loading-and-inspection\"></a>  \n",
    "\n",
    "In this section, we will load the datasets required to build and evaluate our recommender system. By inspecting the structure of the data, we can gain insights into its **quality** and **completeness**. This will guide our **data cleaning process** to ensure consistency and accuracy in our analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Loading  \n",
    "The datasets are sourced from a **Kaggle competition** and stored as CSV files. We will read them into **Pandas DataFrames** for easier manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Define the paths to your data files\n",
    "anime_path = \"Data/anime.csv\"\n",
    "ratings_path = \"Data/train.csv\"  \n",
    "test_path = \"Data/test.csv\"\n",
    "submission_path = \"Data/submission.csv\"  \n",
    "\n",
    "# Load the datasets\n",
    "anime_df = pd.read_csv(anime_path)\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "submission_df = pd.read_csv(submission_path)  \n",
    "\n",
    "# Confirm data loading\n",
    "print(\"\\033[1;35mDatasets loaded successfully!\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖWe will duplicate our anime dataset to avoid contaminating the original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"anime.csv\" file into two separate DataFrames\n",
    "# anime_df1 = pd.read_csv(anime_path)\n",
    "# anime_df2 = pd.read_csv(anime_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Inspection \n",
    "Data inspection is the process of understanding and assessing the structure, completeness, \n",
    "    and quality of a dataset. This step ensures that the data is ready for further analysis \n",
    "    by identifying missing or inconsistent values, understanding data types, and spotting any \n",
    "    potential anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖPreview the anime Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Dataset Dimensions: {anime_df.shape}\\n\")\n",
    "display(anime_df.head(2).style.set_properties(**{\"border\": \"1.5px solid black\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖPreview the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Dataset Dimensions: {ratings_df.shape}\\n\")\n",
    "display(ratings_df.head(2).style.set_properties(**{\"border\": \"1.5px solid black\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖPreview the Submission Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Dimensions: {test_df.shape}\\n\")\n",
    "display(test_df.head(2).style.set_properties(**{\"border\": \"1.5px solid black\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖPreview the test Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Dimensions: {submission_df.shape}\\n\")\n",
    "display(submission_df.head(2).style.set_properties(**{\"border\": \"1.5px solid black\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üìÇ 4. Data Cleaning  üßπ\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "<a id=\"4-data-cleaning\"></a>  \n",
    "Data cleaning is the process of identifying and correcting errors, inconsistencies, and missing values in a dataset. It ensures that our data is accurate, consistent, and ready for analysis or modeling. Poor data quality can lead to misleading insights and inaccurate predictions, making this step crucial.  \n",
    "\n",
    "In this section, we will:  \n",
    "‚úÖ Identify and handle **missing values**  \n",
    "‚úÖ Detect and remove **duplicate records**  \n",
    "‚úÖ Standardize **inconsistent formatting**  \n",
    "‚úÖ Address **outliers and anomalies**  \n",
    "‚úÖ Ensure data types are correctly assigned  \n",
    "\n",
    "By the end of this process, we will have a clean and structured dataset that is ready for building our recommender system.  \n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check and handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖCheck missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_missing_values(df, name):\n",
    "    \"\"\"Displays missing values in a DataFrame in a clean, readable format.\"\"\"\n",
    "    missing = df.isnull().sum().to_frame(name=\"Missing Values\")\n",
    "    \n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    display(missing.style.set_properties(**{\n",
    "        \"border\": \"1px solid black\", \n",
    "        \"text-align\": \"center\"\n",
    "    }).set_table_styles([{\"selector\": \"th\", \n",
    "\"props\": [(\"background-color\", \"#f4f4f4\"), (\"font-weight\", \"bold\")]\n",
    "    }]))\n",
    "print(\"üîç Missing Values in Each Dataset:\")\n",
    "datasets = {\n",
    "    \"Anime\": anime_df,\n",
    "    \"Ratings\": ratings_df,\n",
    "    \"Test\": test_df\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    display_missing_values(df, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖDisplay Info of Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nAnime Dataset Info:\")\n",
    "print(anime_df.info())\n",
    "print(\"\\nRatings Dataset Info:\")\n",
    "print(ratings_df.info())\n",
    "print(\"\\nTest Dataset Info:\")\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖHandling Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df['rating'] = anime_df['rating'].fillna(anime_df.groupby('type')['rating'].transform('mean'))\n",
    "anime_df['genre'] = anime_df['genre'].fillna('Unknown')\n",
    "anime_df['name'] = anime_df['name'].fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖCleaning Text Data (train_name & train_genre):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_df1['train_genre'] = (\n",
    "#     anime_df1['genre'] + \" \" \n",
    "# )\n",
    "# anime_df2['train_name'] = (\n",
    "#     anime_df2['name'] + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def clean_text(text):\n",
    "#     text = str(text) \n",
    "#     text = text.lower() \n",
    "#     text = re.sub(r'\\d+', '', text)\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#     return text\n",
    "# anime_df1['cleaned_genre'] = anime_df1['train_genre'].apply(clean_text)\n",
    "# anime_df2['cleaned_name'] = anime_df2['train_name'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization to find modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeniser = TreebankWordTokenizer()\n",
    "# anime_df1['genre_tokens'] = anime_df1['cleaned_genre'].apply(tokeniser.tokenize)\n",
    "# anime_df2['name_tokens'] = anime_df2['cleaned_name'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # Function to get top N words in a category\n",
    "# def get_top_genre(type1, n=3):\n",
    "#     type_tokens = anime_df1[anime_df1['type'] == type1]['genre_tokens'].explode().tolist()\n",
    "#     word_counts = Counter(type_tokens)\n",
    "#     return word_counts.most_common(n)\n",
    "\n",
    "# # Plot top 10 words for each category\n",
    "# types= anime_df1['type'].unique()\n",
    "# num_types = len(types)\n",
    "\n",
    "# plt.figure(figsize=(16, 4 * (num_types // 3 + 1)))  # Adjust height dynamically\n",
    "\n",
    "# for i, type1 in enumerate(types, 1):\n",
    "#     plt.subplot((num_types // 3) + 1, 3, i)\n",
    "#     top_words = get_top_genre(type1)\n",
    "    \n",
    "#     if not top_words:\n",
    "#         continue  # Skip empty categories\n",
    "    \n",
    "#     words, counts = zip(*top_words)\n",
    "#     sns.barplot(x=list(counts), y=list(words), palette='coolwarm')\n",
    "    \n",
    "#     plt.title(f'Top 3 Genres in {type1.capitalize()}')\n",
    "#     plt.xlabel('Frequency')\n",
    "#     plt.ylabel('Words')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖThe above plot displays the top 3 genres per type. The highest genre per type will be the mode for that specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''most_frequent_type = anime_df1['type'].value_counts().idxmax()\n",
    "most_frequent_count = anime_df1['type'].value_counts().max()\n",
    "print(f\"The most frequent type is '{most_frequent_type}' with {most_frequent_count} occurrences.\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖTV is the type with the highest frequency. TV is our mode in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Replacing Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Null-genre entries\n",
    "anime_df['genre'] = anime_df.apply(lambda row: 'Comedy' if row['type'] == 'Movie' and (not row['genre'] or pd.isna(row['genre'])) else row['genre'], axis=1)\n",
    "anime_df['genre'] = anime_df.apply(lambda row: 'Comedy' if row['type'] == 'TV' and (not row['genre'] or pd.isna(row['genre'])) else row['genre'], axis=1)\n",
    "anime_df['genre'] = anime_df.apply(lambda row: 'Hentai' if row['type'] == 'OVA' and (not row['genre'] or pd.isna(row['genre'])) else row['genre'], axis=1)\n",
    "anime_df['genre'] = anime_df.apply(lambda row: 'Comedy' if row['type'] == 'Special' and (not row['genre'] or pd.isna(row['genre'])) else row['genre'], axis=1)\n",
    "anime_df['genre'] = anime_df.apply(lambda row: 'Music' if row['type'] == 'Music' and (not row['genre'] or pd.isna(row['genre'])) else row['genre'], axis=1)\n",
    "anime_df['genre'] = anime_df.apply(lambda row: 'Comedy' if row['type'] == 'ONA' and (not row['genre'] or pd.isna(row['genre'])) else row['genre'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Null-type entries\n",
    "anime_df['type'] = anime_df['type'].replace('', 'TV').fillna('TV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Null-ratings entries\n",
    "average_rating_per_type = anime_df.groupby('type')['rating'].transform('mean')\n",
    "anime_df['rating'] = anime_df['rating'].fillna(average_rating_per_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAY!!!There are no missing values anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_df = anime_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üìä 5. Exploratory Data Analysis (EDA) üîç\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "<a id=\"5-exploratory-data-analysis\"></a>  \n",
    "Exploratory Data Analysis (EDA) is the process of analyzing and visualizing datasets to uncover patterns, trends, and relationships. It helps us understand the structure of our data, detect anomalies, and generate insights before applying machine learning models.  \n",
    "\n",
    "In this section, we will:  \n",
    "‚úÖ Summarize and visualize **key statistics**  \n",
    "‚úÖ Identify **missing values** and their distribution  \n",
    "‚úÖ Explore **data distributions** and **correlations**  \n",
    "‚úÖ Detect and handle **outliers and anomalies**  \n",
    "‚úÖ Understand the **relationships between features**  \n",
    "\n",
    "By the end of this process, we will have a deeper understanding of our dataset, enabling us to make informed decisions for building our recommender system.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖSummary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for anime dataset\n",
    "print(\"Summary Statistics for Anime Dataset:\")\n",
    "print(anime_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Summary statistics for ratings dataset\n",
    "print(\"Summary Statistics for User Ratings (Train) Dataset:\")\n",
    "print(ratings_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Summary statistics for test dataset\n",
    "print(\"Summary Statistics for Test Dataset:\")\n",
    "print(test_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Data info for anime dataset\n",
    "print(\"Data Info for Anime Dataset:\")\n",
    "print(anime_df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Data info for ratings dataset\n",
    "print(\"Data Info for User Ratings (Train) Dataset:\")\n",
    "print(ratings_df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Data info for test dataset\n",
    "print(\"Data Info for Test Dataset:\")\n",
    "print(test_df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Unique value counts for each dataset\n",
    "print(\"Unique Value Counts in Anime Dataset:\")\n",
    "print(anime_df.nunique())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Unique Value Counts in User Ratings (Train) Dataset:\")\n",
    "print(ratings_df.nunique())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Unique Value Counts in Test Dataset:\")\n",
    "print(test_df.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  User-related: User behavior analysis\n",
    "This section will delve into how users interact with the anime data, providing insights for building an effective recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ\n",
    "* Distribution of ratings:\n",
    "    - This will help us understand user preferences and identify potential biases (e.g., skewed towards high or low ratings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of user ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ratings_df['rating'], bins=20, kde=False, color='skyblue')\n",
    "plt.title('Distribution of User Ratings')\n",
    "plt.xlabel('User Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ratings are skewed towards high ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of ratings per user\n",
    "user_ratings_count = ratings_df.groupby('user_id')['rating'].count()\n",
    "\n",
    "# Descriptive statistics for the distribution of user ratings\n",
    "print(\"Descriptive Statistics for Number of Ratings per User:\")\n",
    "print(user_ratings_count.describe())\n",
    "\n",
    "# Visualize the distribution of ratings per user\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(user_ratings_count, bins=50, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Ratings per User')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖNumber of unique users and anime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_anime = ratings_df['anime_id'].nunique()\n",
    "print(f'Number of unique users: {num_users}')\n",
    "print(f'Number of unique anime: {num_anime}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖIdentify active and inactive users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of ratings per user\n",
    "user_ratings_count = ratings_df.groupby('user_id')['rating'].count()\n",
    "active_threshold = user_ratings_count.quantile(0.75)  # 75th percentile\n",
    "inactive_threshold = user_ratings_count.quantile(0.25)  # 25th percentile\n",
    "active_users = user_ratings_count[user_ratings_count > active_threshold].index\n",
    "inactive_users = user_ratings_count[user_ratings_count < inactive_threshold].index\n",
    "print(f\"Number of active users: {len(active_users)}\")\n",
    "print(f\"Number of inactive users: {len(inactive_users)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 69,481 total users, but only 34,614 (17,326 active + 17,288 inactive) are accounted for\n",
    "in our user activity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖCategorizing Users Based on Activity Levels and Visualizing the Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_ratings_count = ratings_df.groupby('user_id')['rating'].count()\n",
    "very_active_threshold = user_ratings_count.quantile(0.9)  # Top 10%\n",
    "active_threshold = user_ratings_count.quantile(0.75)      # Top 25%\n",
    "inactive_threshold = user_ratings_count.quantile(0.25)    # Bottom 25%\n",
    "very_inactive_threshold = user_ratings_count.quantile(0.1)  # Bottom 10%\n",
    "user_activity = pd.DataFrame({'user_id': user_ratings_count.index})\n",
    "user_activity['user_type'] = np.where(\n",
    "    user_ratings_count > very_active_threshold, 'very_active',\n",
    "    np.where(user_ratings_count > active_threshold, 'active',\n",
    "             np.where(user_ratings_count < inactive_threshold, 'inactive', 'low_activity'))\n",
    ")\n",
    "\n",
    "print(\"User Type Distribution:\")\n",
    "print(user_activity['user_type'].value_counts())\n",
    "user_type_order = ['low_activity', 'inactive', 'active', 'very_active']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='user_type', data=user_activity, order=user_type_order, palette='pastel')\n",
    "plt.title('Distribution of User Activity Levels')\n",
    "plt.xlabel('User Type')\n",
    "plt.ylabel('Count of Users')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖComparing Number of Ratings and User Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with user_id, rating_count, and user_type columns\n",
    "user_activity = pd.DataFrame({\n",
    "    'user_id': user_ratings_count.index,\n",
    "    'rating_count': user_ratings_count.values\n",
    "})\n",
    "\n",
    "# Assign user types based on thresholds\n",
    "user_activity['user_type'] = np.where(\n",
    "    user_ratings_count > very_active_threshold, 'very_active',\n",
    "    np.where(user_ratings_count > active_threshold, 'active',\n",
    "             np.where(user_ratings_count < inactive_threshold, 'inactive', 'low_activity')))\n",
    "user_type_order = ['low_activity', 'inactive', 'active', 'very_active']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    x='user_type',\n",
    "    y='rating_count',\n",
    "    data=user_activity,\n",
    "    order=user_type_order,\n",
    "    showmeans=True,\n",
    "    palette='pastel'\n",
    ")\n",
    "plt.title('Comparing Number of Ratings and User Type')\n",
    "plt.xlabel('User Type')\n",
    "plt.ylabel('Number of Ratings')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyzed the average rating behavior of active (active & very active) vs. inactive (low_activitiy & inactive) users. It helps understand if these groups have systematic differences in their rating patterns, which can be valuable for recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Anime Content Behavior: Insights into Genres, Ratings, and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of average anime ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=anime_df, x='rating', bins=20, kde=False, color='salmon')\n",
    "plt.title('Distribution of Average Anime Ratings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Anime Rating', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖDistribution of anime types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=anime_df, x='type', order=anime_df['type'].value_counts().index, palette='pastel')\n",
    "plt.title('Distribution of Anime Types', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Anime Type', fontsize=12)\n",
    "plt.ylabel('Count of Anime', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚úÖAnalysis of genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_count = anime_df['genre'].str.get_dummies(sep=', ').sum()\n",
    "plt.figure(figsize=(12, 6))\n",
    "genre_count.sort_values(ascending=False).plot(kind='bar', color='lightblue')\n",
    "plt.title('Distribution of Anime Genres', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.ylabel('Count of Anime', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚úÖTop 10 genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genres = anime_df['genre'].str.get_dummies(sep=', ').sum().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_genres.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Anime Genres', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Genre', fontsize=12)\n",
    "plt.ylabel('Count of Anime', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚úÖTop 10 anime by number of user ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_anime_by_ratings = ratings_df['anime_id'].value_counts().head(10)\n",
    "top_anime_titles = anime_df.set_index('anime_id').loc[top_anime_by_ratings.index]['name']\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(y=top_anime_titles, x=top_anime_by_ratings.values, orient='h', palette='coolwarm')\n",
    "plt.title('Top 10 Anime by Number of User Ratings', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Ratings', fontsize=12)\n",
    "plt.ylabel('Anime Title', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚úÖTop 10 anime by average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the top 10 anime by average rating from anime_df\n",
    "top_anime_by_avg_rating = anime_df[['anime_id', 'name', 'rating']].drop_duplicates('anime_id') \\\n",
    "    .sort_values(by='rating', ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='rating',\n",
    "    y='name',\n",
    "    data=top_anime_by_avg_rating,\n",
    "    orient='h',\n",
    "    palette='coolwarm'\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Top 10 Anime by Average Rating', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Average Rating', fontsize=12)\n",
    "plt.ylabel('Anime Title', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the top 10 list for reference\n",
    "print(\"Top 10 Anime by Average Rating:\")\n",
    "print(top_anime_by_avg_rating[['name', 'rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚úÖTop rated animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the top 10 unique animes by user rating\n",
    "top_rated_unique_animes = ratings_df.groupby('anime_id')['rating'].mean()\n",
    "subset = pd.DataFrame(anime_df[['anime_id', 'name']])\n",
    "subset['rating'] = subset['anime_id'].map(top_rated_unique_animes)\n",
    "subset = subset.nlargest(10, 'rating')\n",
    "\n",
    "# Visualize top 10 unique user rated animes\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(\n",
    "    subset['name'],\n",
    "    subset['rating'],\n",
    "    color='skyblue'\n",
    ")\n",
    "plt.title('Top 10 Unique Anime by User Rating')\n",
    "plt.xlabel('User Rating')\n",
    "plt.ylabel('Anime Title')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖGet the top 10 unique genre by user rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_animes_exploded = unique_animes.assign(genre=unique_animes['genre'].str.split(', ')).explode('genre')\n",
    "# top_genres = unique_animes_exploded.groupby('genre')['rating_x'].mean().reset_index()\n",
    "top_genres = anime_df.nlargest(10, 'rating')\n",
    "# top_genres = top_genres.rename(columns={'rating_x': 'average_anime_rating'})\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(\n",
    "    top_genres['genre'],\n",
    "    top_genres['rating'],\n",
    "    color='lightblue'\n",
    ")\n",
    "plt.title('Top 10 Unique Genres by Anime Rating')\n",
    "plt.xlabel('Average Anime Rating')\n",
    "plt.ylabel('Genre')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlation Matricx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(ratings_df, anime_df[['anime_id', 'rating', 'episodes', 'members']], on='anime_id', how='left')\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'rating_x': 'user_rating',  \n",
    "    'rating_y': 'average_anime_rating' \n",
    "})\n",
    "numeric_cols = ['user_rating', 'average_anime_rating', 'episodes', 'members']\n",
    "merged_df['episodes'] = pd.to_numeric(merged_df['episodes'], errors='coerce')  \n",
    "correlation_matrix = merged_df[numeric_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    üìä 6. Data preprocessing\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    " \n",
    "<b>Data preprocessing</b> is a crucial step in building any machine learning model. It involves transforming raw data into a clean, structured format that is suitable for analysis. This step includes various tasks such as encoding categorical variables, normalizing data, and feature extraction. Proper data preprocessing ensures that the data is of high quality, which in turn improves the performance of the machine learning models. This involves encoding of categorical\n",
    "variables, ensuring uniformity and compatibility for subsequent analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = merged_df.copy()  \n",
    "numeric_cols = ['episodes', 'members']\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df['scaled_score'] = scaler.fit_transform(df[['average_anime_rating']])\n",
    "\n",
    "# Function to preprocess features\n",
    "def preprocess_features(df):\n",
    "    # Combine relevant features for vectorization\n",
    "    combined_features = df[[\"genre\", \"type\", \"episodes\"]].fillna(\"\").astype(str)\n",
    "    combined_features = combined_features.apply(lambda x: ' '.join(x), axis=1)\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "rec_data = anime_df.drop_duplicates(subset=\"name\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# Assuming rec_data DataFrame is already defined and preprocessed as in the original code\n",
    "# Example: rec_data = pd.read_csv(\"anime_data.csv\")  # Load your dataset\n",
    "\n",
    "combined_features = preprocess_features(rec_data)\n",
    "\n",
    "# Prepare combined features\n",
    "tfv = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_features=None,\n",
    "    strip_accents=\"unicode\",\n",
    "    analyzer=\"word\",\n",
    "    token_pattern=r\"\\w{1,}\",\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=\"english\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"border: 2px solid #ff4d6d; padding: 15px; border-radius: 10px; \n",
    "                           box-shadow: 0px 0px 10px rgba(255, 77, 109, 0.7); width: 80%;\">\n",
    "  \n",
    "  <h2 style=\"font-family: 'Comic Sans MS', cursive; color: #ff4d6d;\">\n",
    "    ü§ñ 6. Model Development üöÄ\n",
    "  </h2>\n",
    "\n",
    "</div>\n",
    "\n",
    "<a id=\"Model Development\"></a>\n",
    "\n",
    "### **Model Development**\n",
    "Model development is the process of designing, training, and optimizing machine learning models to achieve high predictive accuracy. This stage involves selecting the appropriate algorithms, tuning hyperparameters, and evaluating model performance to ensure optimal results.  \n",
    "\n",
    "#### **In this section, we will:**\n",
    "‚úÖ **Select and implement machine learning models** suitable for our recommendation system  \n",
    "‚úÖ **Train the models** using preprocessed data to learn patterns and relationships  \n",
    "‚úÖ **Optimize hyperparameters** to enhance performance and prevent overfitting  \n",
    "‚úÖ **Evaluate model performance** using appropriate metrics such as Log Loss, RMSE, or accuracy  \n",
    "‚úÖ **Compare different models** to identify the best-performing approach  \n",
    "\n",
    "By the end of this process, we will have a well-trained and validated recommendation model, ready for deployment and further fine-tuning.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "\n",
    "\n",
    "# Fit and transform the combined features\n",
    "tfidf_matrix = tfv.fit_transform(combined_features)\n",
    "\n",
    "# Compute the sigmoid kernel\n",
    "sig = sigmoid_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Create a series with anime indices\n",
    "indices = pd.Series(rec_data.index, index=rec_data[\"name\"]).drop_duplicates()\n",
    "\n",
    "# Recommendation Function\n",
    "def give_rec(title, sig=sig):\n",
    "    if title not in indices:\n",
    "        return f\"‚ùå '{title}' not found in the dataset!\"\n",
    "    \n",
    "    idx = indices[title]\n",
    "    sig_scores = list(enumerate(sig[idx]))\n",
    "    sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True)\n",
    "    sig_scores = sig_scores[1:11]  # Top 10 recommendations (excluding itself)\n",
    "    \n",
    "    anime_indices = [i[0] for i in sig_scores]\n",
    "    \n",
    "    # Return the result as a formatted DataFrame\n",
    "    return pd.DataFrame({\n",
    "        \"No\": range(1, 11),\n",
    "        \"Anime Name\": rec_data[\"name\"].iloc[anime_indices].values,\n",
    "        \"Rating\": rec_data[\"rating\"].iloc[anime_indices].values\n",
    "    }).set_index(\"No\")\n",
    "\n",
    "# Example usage\n",
    "recommendations = give_rec(\"Naruto\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖData Splitting\n",
    "\n",
    "\n",
    "Here, we split the dataset df_merged into training and validation sets. We use 80% of the data for training and 20% for validation to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_ratings_df = ratings_df.groupby('anime_id')['rating'].mean()\n",
    "avg_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = pd.merge(test_df, avg_ratings_df, on='anime_id', how='left')\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model['rating'].fillna(base_model['rating'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model['ID'] = base_model['user_id'].astype(str) + '_' + base_model['anime_id'].astype(str)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = base_model[['ID', 'rating']]\n",
    "base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fillna(base_model['rating'].mean(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.to_csv('first_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "train, val = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "avg_ratings_train = train.groupby('anime_id')['rating'].mean()\n",
    "val_preds = pd.merge(val, avg_ratings_train, on='anime_id', how='left')\n",
    "val_preds['rating_y'] = val_preds['rating_y'].fillna(train['rating'].mean())\n",
    "rmse = np.sqrt(mean_squared_error(val_preds['rating_x'], val_preds['rating_y']))\n",
    "\n",
    "print(f\"Baseline RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
